{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning a model on a Q&A task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "import datasets as ds\n",
        "from datasets import load_metric, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pyarrow.parquet as pq\n",
        "import Levenshtein\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset package\n",
        "SupremeCourtOfIsrael = ds.load_dataset('LevMuchnik/SupremeCourtOfIsrael')\n",
        "\n",
        "# Convert the datasets to Dataframe (9 Min)\n",
        "Hugging_Face_df = pd.DataFrame.from_dict(SupremeCourtOfIsrael['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset package locally (30 Sec)\n",
        "Hugging_Face_df = pq.read_table(source='./SupremeCourtOfIsrael/cases_all.parquet').to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find Legal Clauses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the list of Basic Laws\n",
        "basic_laws = [\n",
        "    'חוק-יסוד הכנסת',\n",
        "    'חוק-יסוד מקרקעי ישראל',\n",
        "    'חוק-יסוד נשיא המדינה',\n",
        "    'חוק-יסוד משק המדינה',\n",
        "    'חוק-יסוד הצבא',\n",
        "    'חוק-יסוד ירושלים בירת ישראל',\n",
        "    'חוק-יסוד השפיטה',\n",
        "    'חוק-יסוד מבקר המדינה',\n",
        "    'חוק-יסוד כבוד האדם וחירותו',\n",
        "    'חוק-יסוד חופש העיסוק',\n",
        "    'חוק-יסוד הממשלה',\n",
        "    'חוק-יסוד משאל עם',\n",
        "    'חוק-יסוד ישראל מדינת הלאום של העם היהודי'\n",
        "]\n",
        "\n",
        "# Prepare the regex pattern for Basic Laws\n",
        "formats = [re.escape(law.split(' ')[1]) for law in basic_laws]  # Get the part after 'חוק-יסוד'\n",
        "basic_laws_pattern = r'\\bחוק-יסוד\\s*[-: ]?\\s*(?:' + '|'.join(formats) + r')\\b'\n",
        "\n",
        "# Define regex of legal clauses\n",
        "legal_clauses_pattern = r'(?:תקנה|תקנות|סעיף|חוק|הלכה|פסיקה|פסיקת|צו|פקודה|פקודת|כלל|כללי|כללים|הוראה|הוראות)\\s[א-ת\\s–\"\\',()\\[\\]\\-]*?(?:-|:|\\s)?(?:ת[א-ת]{1,2}\\s?\\d{4}|\\d{4})'\n",
        "\n",
        "# Combine the patterns into one pattern that will match both legal clauses and basic Laws\n",
        "combined_pattern = rf'{legal_clauses_pattern}|{basic_laws_pattern}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_legal_text(text):\n",
        "    # Find all legal text matches using the compiled combined pattern\n",
        "    legal_matches = re.findall(combined_pattern, text, re.VERBOSE)\n",
        "    \n",
        "    # Filter out matches longer than 30 words\n",
        "    legal_matches = [match.strip() for match in legal_matches if len(match.split()) <= 30]\n",
        "    \n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    unique_matches = []\n",
        "    for match in legal_matches:\n",
        "        if all(Levenshtein.distance(match, existing) > 2 for existing in seen):\n",
        "            unique_matches.append(match)\n",
        "            seen.add(match)\n",
        "    \n",
        "    return unique_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Len: 750841\n"
          ]
        }
      ],
      "source": [
        "# Remove rows with empty \"text\" column\n",
        "Hugging_Face_df = Hugging_Face_df[Hugging_Face_df['text'].isna() == False]\n",
        "print(\"Len:\", len(Hugging_Face_df))\n",
        "\n",
        "# Apply find_legal_text function to the 'text' column and store the result in a new column (3 min)\n",
        "Hugging_Face_df['Legal_Clauses_Found'] = Hugging_Face_df['text'].apply(find_legal_text)\n",
        "\n",
        "# Save only rows with legal clauses or precedents found in the \"text\" column\n",
        "legal_df = Hugging_Face_df[(Hugging_Face_df['Legal_Clauses_Found'].apply(len) > 0)][['text', 'Legal_Clauses_Found']]\n",
        "\n",
        "\n",
        "# Arange df\n",
        "# --------------------------\n",
        "\n",
        "# Rename columns\n",
        "legal_df.rename(columns={'text': \"context\", 'Legal_Clauses_Found': 'answers'}, inplace=True)\n",
        "\n",
        "# Create id column\n",
        "legal_df['id'] = legal_df.index\n",
        "\n",
        "# Create new column\n",
        "legal_df['question'] = 'אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?'\n",
        "\n",
        "# Reorder columns\n",
        "legal_df = legal_df[['id', 'context', 'question', 'answers']]\n",
        "\n",
        "# Replace newline characters with space\n",
        "legal_df['context'] = legal_df['context'].str.replace(\"\\n\", \" \")\n",
        "\n",
        "\n",
        "# Find 'start_indices' and create a dataset\n",
        "# ------------------------------------------\n",
        "\n",
        "# Find the start indices of all the elements of the answers list in the context column\n",
        "def find_start_indices(row):\n",
        "    indices = []\n",
        "    for answer in row['answers']:\n",
        "        indices.append(row['context'].find(answer))\n",
        "    return indices\n",
        "\n",
        "# Apply the function to the df to get the indices of the start of both elements in the context column\n",
        "legal_df['start_indices'] = legal_df.apply(find_start_indices, axis=1)\n",
        "\n",
        "# Convert the 'answers' col to a dictionary\n",
        "def convert_to_dict(answers, start_indices):\n",
        "    return {'text': answers, 'answer_start': start_indices}\n",
        "\n",
        "# Apply the function to each row of the df\n",
        "legal_df['answers'] = legal_df.apply(lambda row: convert_to_dict(row['answers'], row['start_indices']), axis=1)\n",
        "\n",
        "# Drop 'start_indices' col\n",
        "legal_df.drop(['start_indices'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find Precedents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define initial letters that might precede the prefixes\n",
        "initial_letters = r'(?:\\b(?:ב|וב|ה|וה)\\b)?'  # Optional initial letters\n",
        "\n",
        "additional_prefixes = [\n",
        "    \"אב\\\"ע\", \"א\\\"ת\", \"את\\\"פ\", \"אמ\\\"ץ\", \"פ\\\"ר\", \"אפ\\\"ח\", \"א\\\"פ\", \"ב\\\"ל\", \"וח\\\"ק\",\n",
        "    \"בק\\\"מ\", \"ת\\\"ת\", \"ביד\\\"מ\", \"בדמ\\\"ש\", \"בע\\\"ק\", \"בפ\\\"מ\", \"עה\\\"פ\", \"בה\\\"ן\", \"בה\\\"פ\",\n",
        "    \"בפ\\\"ת\", \"בש\\\"ע\", \"בת\\\"ת\", \"בב\\\"נ\", \"בע\\\"א\", \"בר\\\"ש\", \"בר\\\"ע\", \"שב\\\"ד\",\n",
        "    \"שנ\\\"א\", \"גמ\\\"ר\", \"דמ\\\"ר\", \"דמ\\\"ש\", \"דנ\\\"א\", \"דנג\\\"ץ\", \"דנ\\\"מ\", \"דנ\\\"פ\",\n",
        "    \"ד\\\"ט\", \"הס\\\"ת\", \"המ\\\"ע\", \"ה\\\"כ\", \"ה\\\"ת\", \"ה\\\"ט\", \"ה\\\"נ\", \"ה\\\"פ\", \"הפ\\\"ב\",\n",
        "    \"הד\\\"פ\", \"ה\\\"ד\", \"ת\\\"ט\", \"תה\\\"ן\", \"ו\\\"ע\", \"ח\\\"א\", \"חב\\\"ר\", \"חע\\\"מ\", \"חע\\\"ק\",\n",
        "    \"ח\\\"ד\", \"ח\\\"נ\", \"חס\\\"מ\", \"י\\\"ס\", \"כ\\\"צ\", \"מק\\\"מ\", \"מ\\\"י\", \"מי\\\"ב\", \"מ\\\"מ\",\n",
        "    \"מ\\\"ת\", \"מ\\\"ח\", \"נע\\\"ד\", \"ס\\\"ע\", \"ס\\\"ק\", \"סק\\\"כ\", \"פ\\\"ל\", \"עמ\\\"א\", \"ע\\\"א\",\n",
        "    \"עב\\\"ז\", \"ע\\\"ב\", \"עב\\\"ל\", \"עח\\\"ר\", \"ע\\\"נ\", \"ער\\\"מ\", \"עמ\\\"ח\", \"על\\\"ע\", \"עמ\\\"נ\",\n",
        "    \"ע\\\"מ\", \"עמ\\\"מ\", \"עש\\\"מ\", \"עמ\\\"ש\", \"ענ\\\"א\", \"ענ\\\"פ\", \"ענמ\\\"ש\", \"עס\\\"ק\", \"ע\\\"ע\",\n",
        "    \"עב\\\"י\", \"עמל\\\"ע\", \"עמש\\\"מ\", \"עמר\\\"מ\", \"ער\\\"פ\", \"ע\\\"ר\", \"עמ\\\"פ\", \"עש\\\"ר\", \"ע\\\"ו\",\n",
        "    \"על\\\"ח\", \"עק\\\"נ\", \"עק\\\"פ\", \"עע\\\"מ\", \"עעת\\\"א\", \"ע\\\"פ\", \"עפ\\\"א\", \"עפ\\\"ג\", \"עפ\\\"ר\",\n",
        "    \"עפ\\\"ת\", \"עפס\\\"פ\", \"עפ\\\"ס\", \"עש\\\"א\", \"ע\\\"ש\", \"עש\\\"ת\", \"ע\\\"ח\", \"עב\\\"פ\", \"עא\\\"פ\",\n",
        "    \"עח\\\"ע\", \"עע\\\"ר\", \"עפ\\\"ע\", \"עה\\\"ג\", \"עמ\\\"י\", \"עמ\\\"ת\", \"עכ\\\"ב\", \"עק\\\"מ\", \"עח\\\"ק\",\n",
        "    \"עפ\\\"מ\", \"עפ\\\"ן\", \"בג\\\"ץ\", \"עג\\\"ר\", \"עת\\\"מ\", \"עת\\\"א\", \"פק\\\"ח\", \"פר\\\"ק\", \"פ\\\"ה\",\n",
        "    \"פש\\\"ר\", \"צ\\\"א\", \"צ\\\"ה\", \"צ\\\"ח\", \"מ\\\"כ\", \"צ\\\"ו\", \"ק\\\"פ\", \"ק\\\"ג\", \"רע\\\"ס\", \"רע\\\"א\",\n",
        "    \"רע\\\"מ\", \"רמ\\\"ש\", \"רצ\\\"פ\", \"רע\\\"צ\", \"רע\\\"ו\", \"רע\\\"ב\", \"רעת\\\"א\", \"רע\\\"פ\", \"רע\\\"ש\",\n",
        "    \"רת\\\"ק\", \"ש\\\"ש\", \"ש\", \"ש\\\"ע\", \"ת\\\"ד\", \"נ\\\"ב\", \"תמ\\\"ק\", \"תמ\\\"ר\", \"תנ\\\"ג\", \"ת\\\"ק\",\n",
        "    \"ת\\\"ב\", \"סב\\\"א\", \"גז\\\"ז\", \"ח\\\"ש\", \"תג\\\"א\", \"ת\\\"ח\", \"תנ\\\"ז\", \"תע\\\"א\", \"ת\\\"צ\", \"ת\\\"מ\",\n",
        "    \"תא\\\"מ\", \"תא\\\"ח\", \"תא\\\"ק\", \"ת\\\"א\", \"תה\\\"ג\", \"תה\\\"ס\", \"ע\\\"ל\", \"תל\\\"א\", \"תל\\\"ב\",\n",
        "    \"תל\\\"פ\", \"תמ\\\"ש\", \"ת\\\"ע\", \"ת\\\"פ\", \"תפ\\\"ח\", \"ת\\\"ג\", \"תת\\\"ח\", \"תת\\\"ע\", \"תו\\\"ח\", \"תו\\\"ב\",\n",
        "    \"המ\\\"ש\", \"הע\\\"ז\", \"ש\\\"מ\", \"שע\\\"מ\", \"בש\\\"א\", \"ר\\\"ע\", \"ראו\", \"למשל\", \"בת.פ.\", \"ת.א.\"\n",
        "]\n",
        "\n",
        "# Join the prefixes into a regex pattern, ensuring word boundaries\n",
        "prefix_pattern = r'\\b(?:' + '|'.join(re.escape(prefix) for prefix in additional_prefixes) + r')\\b'\n",
        "\n",
        "# Pattern for numbers (1-8 digits), joined or separated by dashes or slashes\n",
        "number_pattern = r'\\b\\d{1,8}(?:[-/]\\d{1,8})*\\b'\n",
        "\n",
        "# Combine the patterns\n",
        "full_pattern = initial_letters + prefix_pattern + r'[ ,.:;!?]*' + number_pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find legal precedents within the \"text\" column\n",
        "def find_legal_precedents(text):\n",
        "    pattern = full_pattern\n",
        "\n",
        "    # Replace newline characters with space\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    precedents_matches = re.findall(pattern, text)\n",
        "    \n",
        "    # Filter matches to remove those with length exceeding too much characters\n",
        "    filtered_matches = [match for match in precedents_matches if len(match) <= 25]\n",
        "\n",
        "    # Remove duplicate legal clauses\n",
        "    seen = set()\n",
        "    unique_precedents_matches = []\n",
        "    \n",
        "    for match in filtered_matches:\n",
        "        # Remove all spaces and compare\n",
        "        cleaned_match = re.sub(r'\\s+', '', match)\n",
        "        if cleaned_match not in seen:\n",
        "            seen.add(cleaned_match)\n",
        "            unique_precedents_matches.append(match)\n",
        "    \n",
        "    return unique_precedents_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Len: 750841\n"
          ]
        }
      ],
      "source": [
        "# Remove rows with empty \"text\" column\n",
        "Hugging_Face_df = Hugging_Face_df[Hugging_Face_df['text'].isna() == False]\n",
        "print(\"Len:\", len(Hugging_Face_df))\n",
        "\n",
        "# Apply find_legal_precedents function to the 'text' column and store the result in a new column\n",
        "Hugging_Face_df['precedents_found'] = Hugging_Face_df['text'].apply(find_legal_precedents)\n",
        "\n",
        "# Save only rows with precedents found in the \"text\" column\n",
        "legal_df = Hugging_Face_df[Hugging_Face_df['precedents_found'].apply(len) > 0][['text', 'precedents_found']]\n",
        "\n",
        "# Arange df\n",
        "# --------------------------\n",
        "\n",
        "# Rename columns\n",
        "legal_df.rename(columns={'text': \"context\", 'precedents_found': 'answers'}, inplace=True)\n",
        "\n",
        "# Create id column\n",
        "legal_df['id'] = legal_df.index\n",
        "\n",
        "# Create new column\n",
        "legal_df['question'] = 'באילו פסקי דין\\תקדימים נעשה שימוש ?'\n",
        "\n",
        "# Reorder columns\n",
        "legal_df = legal_df[['id', 'context', 'question', 'answers']]\n",
        "\n",
        "# Replace newline characters with space\n",
        "legal_df['context'] = legal_df['context'].str.replace(\"\\n\", \" \")\n",
        "\n",
        "\n",
        "# Find 'start_indices' and create a dataset\n",
        "# ------------------------------------------\n",
        "\n",
        "# Find the start indices of all the elements of the answers list in the context column\n",
        "def find_start_indices(row):\n",
        "    indices = []\n",
        "    for answer in row['answers']:\n",
        "        indices.append(row['context'].find(answer))\n",
        "    return indices\n",
        "\n",
        "# Apply the function to the df to get the indices of the start of both elements in the context column\n",
        "legal_df['start_indices'] = legal_df.apply(find_start_indices, axis=1)\n",
        "\n",
        "# Convert the 'answers' col to a dictionary\n",
        "def convert_to_dict(answers, start_indices):\n",
        "    return {'text': answers, 'answer_start': start_indices}\n",
        "\n",
        "# Apply the function to each row of the df\n",
        "legal_df['answers'] = legal_df.apply(lambda row: convert_to_dict(row['answers'], row['start_indices']), axis=1)\n",
        "\n",
        "# Drop 'start_indices' col\n",
        "legal_df.drop(['start_indices'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>ב בית המשפט העליון   בירושלים    רע...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['תקנות סדר הדין האזרחי,\n",
              "תשמ\"ד1984'],...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>ב בית המשפט העליון  בשבתו כבית משפט  לער...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['כלל לעניינו\n",
              "של המערער, ועל כך עמד ב...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>ב בית המשפט העליון       ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['תקנות סדר הדין האזרחי, התשמ\"ד1984']...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>ב בית המשפט העליון       ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['תקנות\n",
              "סדר הדין האזרחי התשמ\"ד1984'],...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>ב בית המשפט העליון       ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['תקנות\n",
              "סדר הדין האזרחי התשמ\"ד1984'],...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751189</th>\n",
              "      <td>751189</td>\n",
              "      <td>בבית המשפט העליו...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751190</th>\n",
              "      <td>751190</td>\n",
              "      <td>בבית המשפט העליון    ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה\n",
              "...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751191</th>\n",
              "      <td>751191</td>\n",
              "      <td>בבית המשפט העליון    ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['חוק סדר הדין הפלילי (סמכויות\n",
              "  אכיפ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751192</th>\n",
              "      <td>751192</td>\n",
              "      <td>בבית המשפט העליון    ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751193</th>\n",
              "      <td>751193</td>\n",
              "      <td>בבית המשפט העליון    ...</td>\n",
              "      <td>אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?</td>\n",
              "      <td>{'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>184933 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                                            context  \\\n",
              "10          10             ב בית המשפט העליון   בירושלים    רע...   \n",
              "11          11        ב בית המשפט העליון  בשבתו כבית משפט  לער...   \n",
              "17          17                       ב בית המשפט העליון       ...   \n",
              "19          19                       ב בית המשפט העליון       ...   \n",
              "20          20                       ב בית המשפט העליון       ...   \n",
              "...        ...                                                ...   \n",
              "751189  751189                                בבית המשפט העליו...   \n",
              "751190  751190                           בבית המשפט העליון    ...   \n",
              "751191  751191                           בבית המשפט העליון    ...   \n",
              "751192  751192                           בבית המשפט העליון    ...   \n",
              "751193  751193                           בבית המשפט העליון    ...   \n",
              "\n",
              "                                        question  \\\n",
              "10      אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "11      אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "17      אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "19      אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "20      אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "...                                          ...   \n",
              "751189  אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "751190  אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "751191  אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "751192  אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "751193  אילו סעיפים\\חוקים\\תקנות מצויינים במסמך ?   \n",
              "\n",
              "                                                  answers  \n",
              "10      {'text': ['תקנות סדר הדין האזרחי,\n",
              "תשמ\"ד1984'],...  \n",
              "11      {'text': ['כלל לעניינו\n",
              "של המערער, ועל כך עמד ב...  \n",
              "17      {'text': ['תקנות סדר הדין האזרחי, התשמ\"ד1984']...  \n",
              "19      {'text': ['תקנות\n",
              "סדר הדין האזרחי התשמ\"ד1984'],...  \n",
              "20      {'text': ['תקנות\n",
              "סדר הדין האזרחי התשמ\"ד1984'],...  \n",
              "...                                                   ...  \n",
              "751189  {'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה ...  \n",
              "751190  {'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה\n",
              "...  \n",
              "751191  {'text': ['חוק סדר הדין הפלילי (סמכויות\n",
              "  אכיפ...  \n",
              "751192  {'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה ...  \n",
              "751193  {'text': ['חוק סדר הדין הפלילי (סמכויות אכיפה ...  \n",
              "\n",
              "[184933 rows x 4 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "legal_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, validation_df = train_test_split(legal_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_dict(train_df)\n",
        "validation_dataset = Dataset.from_dict(validation_df)\n",
        "datasets = ds.DatasetDict({\"train\":train_dataset,\"validation\":validation_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'context', 'question', 'answers'],\n",
              "        num_rows: 147946\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'context', 'question', 'answers'],\n",
              "        num_rows: 36987\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Instantiate a tokenizer with `AutoTokenizer.from_pretrained` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load this model for Legal_Clauses training\n",
        "modelName = 'shay681/HeBERT_finetuned_Legal_Clauses'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load this model for Precedents training\n",
        "modelName = 'shay681/HeBERT_finetuned_Precedents'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
        "\n",
        "# Assertion to ensure that the tokenizer is a fast tokenizer\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5VjZVraHm5"
      },
      "source": [
        "Hyper Parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "simkVCrEaHm5"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "\n",
        "# For the special case where the model expects padding on the left\n",
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ba72pPUeaHnI"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "Apply the function on all the elements of all the splits in `dataset`.\n",
        "\n",
        "Since the preprocessing changes the number of samples, remove the old columns when applying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(modelName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "Instantiate a `Trainer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "model_name = modelName.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    \"./HeBERT_finetuned_Results/\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=5,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    # save_strategy=\"no\"\n",
        "    # use_cpu=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vksssrpFaHnL"
      },
      "source": [
        "Data collator that will batch the processed examples together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLYzRlUraHnL"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Define a `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "Finetune the model by calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue training from last checkpoint\n",
        "trainer.train(resume_from_checkpoint=\"./HeBERT_finetuned_Results/checkpoint-120000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMZ25nz9aHnM"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTUtVQZEaHnM"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"model_fine_tuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instantiate a `pipeline` for Q&A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "import re\n",
        "\n",
        "def remove_duplicate_answers(answers):\n",
        "    unique_answers = []\n",
        "    seen_answers = set()\n",
        "\n",
        "    for answer in answers:\n",
        "        # Remove excess spaces from the answer\n",
        "        normalized_answer =re.sub(r'\\s+', '', answer['answer'])\n",
        "        # Check if the normalized answer has already been seen\n",
        "        if normalized_answer not in seen_answers:\n",
        "            unique_answers.append(answer)\n",
        "            seen_answers.add(normalized_answer)\n",
        "\n",
        "    return unique_answers\n",
        "\n",
        "\n",
        "def merge_answers(answers):\n",
        "    merged_pairs = []\n",
        "\n",
        "    for pair in combinations(answers, 2):\n",
        "        answer1, answer2 = pair\n",
        "        start1, end1 = answer1['start'], answer1['end']\n",
        "        start2, end2 = answer2['start'], answer2['end']\n",
        "\n",
        "        if (start1 <= start2 <= end1) or (start2 <= start1 <= end2):\n",
        "            # Find the overlapping part\n",
        "            overlap_start = max(start1, start2)\n",
        "            overlap_end = min(end1, end2)\n",
        "            overlap_length = overlap_end - overlap_start\n",
        "            \n",
        "            # Merge the answers by concatenating the non-overlapping parts and the overlapping part once\n",
        "            if start1 <= start2:\n",
        "                merged_text = answer1['answer'] + \" \" + answer2['answer'][overlap_length:]\n",
        "            else:\n",
        "                merged_text = answer2['answer'] + \" \" + answer1['answer'][overlap_length:]\n",
        "\n",
        "            merged_pair = {\n",
        "                'score': min(answer1['score'], answer2['score']),\n",
        "                'start': min(start1, start2),\n",
        "                'end': max(end1, end2),\n",
        "                'answer': merged_text\n",
        "            }\n",
        "            merged_pairs.append(merged_pair)\n",
        "\n",
        "    return merged_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "validation_dataset = datasets['validation']\n",
        "\n",
        "# Initialize the question answering pipeline\n",
        "# for Precedents pipeline: model=\"HeBERT_finetuned_Precedents\"\n",
        "question_answerer = pipeline(\"question-answering\", model=\"HeBERT_finetuned_Legal_Clauses\")\n",
        "\n",
        "# Define the regex pattern\n",
        "pattern = re.compile(full_pattern)\n",
        "\n",
        "def get_multiple_answers(question_answerer, question, context, n_best_size=200):\n",
        "    answers = question_answerer(question=question, context=context, top_k=n_best_size)\n",
        "\n",
        "    # Filter answers \n",
        "    filtered_answers = []\n",
        "    no_match_answers = []\n",
        "    pairs = []\n",
        "\n",
        "    for answer in answers:\n",
        "        # if answer['score'] < 1 and pattern.match(answer['answer']):\n",
        "        if pattern.match(answer['answer']):\n",
        "            filtered_answers.append(answer)\n",
        "        else:\n",
        "            no_match_answers.append(answer)\n",
        "            merged_pairs = merge_answers(no_match_answers)\n",
        "            for pair in merged_pairs:\n",
        "                if pattern.match(pair['answer']):\n",
        "                    pairs.append(pair)\n",
        "\n",
        "    fixed_merged_pairs = remove_duplicate_answers(pairs)\n",
        "    filtered_answers += fixed_merged_pairs\n",
        "    \n",
        "    # Sort answers by their start position to help with the containment check\n",
        "    filtered_answers.sort(key=lambda x: x['start'])\n",
        "\n",
        "    # Remove nested answers\n",
        "    unique_answers = []\n",
        "    for answer in filtered_answers:\n",
        "        if not any(answer['start'] >= prev_answer['start'] and answer['end'] <= prev_answer['end'] for prev_answer in unique_answers):\n",
        "            unique_answers.append(answer)\n",
        "\n",
        "    # Extract the text of unique answers\n",
        "    results = [answer for answer in unique_answers]\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over the validation dataset and make predictions\n",
        "\n",
        "for example in validation_dataset:\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "    predicted_answers = get_multiple_answers(question_answerer, question, context)\n",
        "\n",
        "    print(example['id'], \":\",  predicted_answers)\n",
        "\n",
        "    results.append({\n",
        "        'id': example['id'],\n",
        "        'predicted_answers': predicted_answers\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame and save it to CSV\n",
        "predicted_answers = pd.DataFrame(results)\n",
        "predicted_answers.to_csv('predicted_answers.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare Predictions with Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_answer(answer):\n",
        "    \"\"\"Normalize answer by removing all spaces and specified initial letters from each word, and converting to lowercase.\"\"\"\n",
        "    # Remove all spaces\n",
        "    answer = re.sub(r'\\s+', '', answer)\n",
        "    \n",
        "    # Remove initial letters from each word\n",
        "    words = answer.split()\n",
        "    initial_letters = {'ב', 'ל', 'ו', 'ש', 'כ'}  # Add any other letters to this set as needed\n",
        "    \n",
        "    normalized_words = []\n",
        "    for word in words:\n",
        "        if word and word[0] in initial_letters:\n",
        "            normalized_words.append(word[1:])  # Remove the initial letter\n",
        "        else:\n",
        "            normalized_words.append(word)\n",
        "    \n",
        "    normalized_answer = ''.join(normalized_words)\n",
        "    \n",
        "    return normalized_answer.strip().lower()\n",
        "\n",
        "\n",
        "def evaluate_predictions(validation_dataset, question_answerer):\n",
        "    results = []\n",
        "\n",
        "    for example in validation_dataset:\n",
        "        context = example[\"context\"]\n",
        "        question = example[\"question\"]\n",
        "        ground_truth_answers = example[\"answers\"][\"text\"]\n",
        "\n",
        "        # Get the model predictions\n",
        "        predicted_answers = get_multiple_answers(question_answerer, question, context)\n",
        "\n",
        "        # Normalize ground truth answers\n",
        "        normalized_ground_truth = [normalize_answer(ans) for ans in ground_truth_answers]\n",
        "        \n",
        "        # Normalize predicted answers\n",
        "        normalized_predictions = [normalize_answer(pred['answer']) for pred in predicted_answers]\n",
        "\n",
        "        # Check if any of the predicted answers match any of the ground truth answers\n",
        "        matched = any(any(gt in pred for pred in normalized_predictions) for gt in normalized_ground_truth)\n",
        "        # matched = any(normalize_answer(pred['answer']) in normalized_ground_truth for pred in predicted_answers)\n",
        "\n",
        "        # Calculate BLEU scores\n",
        "        bleu_scores = [sentence_bleu([normalize_answer(gt).split()], normalize_answer(pred['answer']).split()) for gt in ground_truth_answers for pred in predicted_answers]\n",
        "\n",
        "        results.append({\n",
        "            'id': example['id'],\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'ground_truth_answers': ground_truth_answers,\n",
        "            'predicted_answers': [pred['answer'] for pred in predicted_answers],\n",
        "            'matched': matched,\n",
        "            'bleu_scores': bleu_scores\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(evaluation_results):\n",
        "    total = len(evaluation_results)\n",
        "    matched = sum(result['matched'] for result in evaluation_results)\n",
        "    accuracy = matched / total\n",
        "\n",
        "    all_bleu_scores = [score for result in evaluation_results for score in result['bleu_scores']]\n",
        "    average_bleu_score = sum(all_bleu_scores) / len(all_bleu_scores) if all_bleu_scores else 0\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Average BLEU Score: {average_bleu_score * 100:.2f}%')\n",
        "\n",
        "    return accuracy, average_bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize Q&A pipeline\n",
        "# for Precedents pipeline: model=\"HeBERT_finetuned_Precedents\"\n",
        "question_answerer = pipeline(\"question-answering\", model=\"HeBERT_finetuned_Legal_Clauses\")\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_predictions(validation_dataset, question_answerer)\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy, average_bleu_score = compute_metrics(evaluation_results)\n",
        "\n",
        "# Convert evaluation results to a DataFrame\n",
        "df_results = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# Save to CSV\n",
        "df_results.to_csv('LegalClauses_evaluation_results.csv', index=False)\n",
        "\n",
        "# for Precedents Evaluation\n",
        "# df_results.to_csv('Precedents_evaluation_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract case number from \"precedents_found\" column\n",
        "def extract_case_number(text):\n",
        "    pattern = r'\\b\\d{1,4}/\\d{1,4}\\b'\n",
        "    case_numbers = re.findall(pattern, text)\n",
        "    return case_numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate BLEU score for each pair of ground truth and predicted answers\n",
        "def calculate_bleu_score(ground_truth, predicted):\n",
        "    if len(predicted) == 0:\n",
        "        return 0  # If there are no predicted answers, return 0 BLEU score\n",
        "    return sentence_bleu([ground_truth], predicted)\n",
        "\n",
        "# Function to calculate Precision, Recall, and F1 score for each pair of ground truth and predicted answers\n",
        "def calculate_prf_scores(ground_truth, predicted):\n",
        "    true_positive = len(set(ground_truth) & set(predicted))\n",
        "    false_positive = len(set(predicted) - set(ground_truth))\n",
        "    false_negative = len(set(ground_truth) - set(predicted))\n",
        "    \n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    return precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "LegalClauses_df = pd.read_csv('./Results/Q&A/LegalClauses_evaluation_results.csv')\n",
        "Precedents_df = pd.read_csv('./Results/Q&A/Precedents_evaluation_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relevant to Precedents_evaluation_results\n",
        "# -----------------------------------------\n",
        "\n",
        "# Apply extract_case_number function to the 'predicted_answers' column and store the result in a new column\n",
        "Precedents_df['gta'] = Precedents_df['ground_truth_answers'].apply(extract_case_number)\n",
        "\n",
        "# Apply extract_case_number function to the 'predicted_answers' column and store the result in a new column\n",
        "Precedents_df['pa'] = Precedents_df['predicted_answers'].apply(extract_case_number)\n",
        "\n",
        "# Remove rows where there are no predicted answers\n",
        "Precedents_df_filtered = Precedents_df[Precedents_df['pa'].apply(len) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU Score: 0.895\n",
            "Average Precision: 0.997\n",
            "Average Recall: 0.967\n",
            "Average F1 Score: 0.970\n"
          ]
        }
      ],
      "source": [
        "# Find Legal Clauses\n",
        "# -----------------\n",
        "# # Apply the function to each row\n",
        "LegalClauses_df['bleu_score'] = LegalClauses_df.apply(lambda row: calculate_bleu_score(row['ground_truth_answers'], row['predicted_answers']), axis=1)\n",
        "\n",
        "# Apply the Precision, Recall, and F1 score function to each row and create separate columns for each\n",
        "LegalClauses_df[['precision', 'recall', 'f1_score']] = LegalClauses_df.apply(lambda row: pd.Series(calculate_prf_scores(row['ground_truth_answers'], row['predicted_answers'])), axis=1)\n",
        "\n",
        "# Calculate the average scores\n",
        "average_bleu_score = LegalClauses_df['bleu_score'].mean()\n",
        "average_precision = LegalClauses_df['precision'].mean()\n",
        "average_recall = LegalClauses_df['recall'].mean()\n",
        "average_f1_score = LegalClauses_df['f1_score'].mean()\n",
        "\n",
        "print(f'Average BLEU Score: {average_bleu_score:.3f}')\n",
        "print(f'Average Precision: {average_precision:.3f}')\n",
        "print(f'Average Recall: {average_recall:.3f}')\n",
        "print(f'Average F1 Score: {average_f1_score:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU Score: 0.516\n",
            "Average Precision: 0.689\n",
            "Average Recall: 0.807\n",
            "Average F1 Score: 0.699\n"
          ]
        }
      ],
      "source": [
        "# Find Precedents\n",
        "# -----------------\n",
        "# Apply the function to each row\n",
        "Precedents_df_filtered['bleu_score'] = Precedents_df_filtered.apply(lambda row: calculate_bleu_score(row['gta'][0], row['pa'][0]), axis=1)\n",
        "\n",
        "# Apply the Precision, Recall, and F1 score function to each row and create separate columns for each\n",
        "Precedents_df_filtered[['precision', 'recall', 'f1_score']] = Precedents_df_filtered.apply(lambda row: pd.Series(calculate_prf_scores(row['gta'], row['pa'])), axis=1)\n",
        "\n",
        "# Calculate the average scores\n",
        "average_bleu_score = Precedents_df_filtered['bleu_score'].mean()\n",
        "average_precision = Precedents_df_filtered['precision'].mean()\n",
        "average_recall = Precedents_df_filtered['recall'].mean()\n",
        "average_f1_score = Precedents_df_filtered['f1_score'].mean()\n",
        "\n",
        "print(f'Average BLEU Score: {average_bleu_score:.3f}')\n",
        "print(f'Average Precision: {average_precision:.3f}')\n",
        "print(f'Average Recall: {average_recall:.3f}')\n",
        "print(f'Average F1 Score: {average_f1_score:.3f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Question Answering on SQUAD",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
