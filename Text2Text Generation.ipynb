{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2Text Generation - mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, TrainingArguments, Trainer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legal_Clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 30.0/30.0 [00:00<00:00, 30.0kB/s]\n",
      "Downloading data: 100%|██████████| 802M/802M [02:17<00:00, 5.84MB/s] \n",
      "Generating train split: 184933 examples [00:33, 5455.70 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset package\n",
    "dataset = load_dataset(\"shay681/Legal_Clauses\")\n",
    "\n",
    "# Convert the datasets to Dataframe (3 Min)\n",
    "filtered_Hugging_Face_df = pd.DataFrame.from_dict(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with extracted Legal_Clauses locally (15 Sec)\n",
    "# filtered_Hugging_Face_df = pd.read_parquet('Hugging_Face_df_Legal_Clauses.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precedents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.07G/1.07G [03:17<00:00, 5.42MB/s]\n",
      "Generating train split: 591506 examples [00:10, 55673.11 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset package\n",
    "dataset = load_dataset(\"shay681/Precedents\")\n",
    "\n",
    "# Convert the datasets to Dataframe (3 Min)\n",
    "filtered_Hugging_Face_df = pd.DataFrame.from_dict(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with extracted Precedents locally (15 Sec)\n",
    "# filtered_Hugging_Face_df = pd.read_parquet('Hugging_Face_df_Precedents.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>Precedents_Found</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>בבית המשפט העליון         ...</td>\n",
       "      <td>[בג\"ץ    5856/03]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159588</td>\n",
       "      <td>בבית המשפט   העל...</td>\n",
       "      <td>[בג\"ץ    5856/03]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160618</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>[בג\"ץ    5856/03]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168038</td>\n",
       "      <td>בבית המשפט   העל...</td>\n",
       "      <td>[בג\"ץ    5856/03]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168411</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>[בג\"ץ    5856/03]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591501</th>\n",
       "      <td>743169</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>[בג\"ץ    6297/22]</td>\n",
       "      <td>751175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591502</th>\n",
       "      <td>743154</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>[בש\"א    6311/22]</td>\n",
       "      <td>751179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591503</th>\n",
       "      <td>743155</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>[בש\"א    6312/22]</td>\n",
       "      <td>751180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591504</th>\n",
       "      <td>743178</td>\n",
       "      <td>בבית המשפט העליון         ...</td>\n",
       "      <td>[בש\"א    6313/22, בש\"א 6039/22]</td>\n",
       "      <td>751181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591505</th>\n",
       "      <td>743176</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>[בש\"א    6314/22, ת\"א 30884-02-22, ת\"א 45854-1...</td>\n",
       "      <td>751182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>591506 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                               text  \\\n",
       "0            1                      בבית המשפט העליון         ...   \n",
       "1       159588                                בבית המשפט   העל...   \n",
       "2       160618                                בבית המשפט העליו...   \n",
       "3       168038                                בבית המשפט   העל...   \n",
       "4       168411                                בבית המשפט העליו...   \n",
       "...        ...                                                ...   \n",
       "591501  743169                                בבית המשפט העליו...   \n",
       "591502  743154                                בבית המשפט העליו...   \n",
       "591503  743155                                בבית המשפט העליו...   \n",
       "591504  743178                      בבית המשפט העליון         ...   \n",
       "591505  743176                                בבית המשפט העליו...   \n",
       "\n",
       "                                         Precedents_Found  __index_level_0__  \n",
       "0                                       [בג\"ץ    5856/03]                  0  \n",
       "1                                       [בג\"ץ    5856/03]                  1  \n",
       "2                                       [בג\"ץ    5856/03]                  2  \n",
       "3                                       [בג\"ץ    5856/03]                  3  \n",
       "4                                       [בג\"ץ    5856/03]                  4  \n",
       "...                                                   ...                ...  \n",
       "591501                                  [בג\"ץ    6297/22]             751175  \n",
       "591502                                  [בש\"א    6311/22]             751179  \n",
       "591503                                  [בש\"א    6312/22]             751180  \n",
       "591504                    [בש\"א    6313/22, בש\"א 6039/22]             751181  \n",
       "591505  [בש\"א    6314/22, ת\"א 30884-02-22, ת\"א 45854-1...             751182  \n",
       "\n",
       "[591506 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_Hugging_Face_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create masked_text\n",
    "def create_masked_text(row):\n",
    "    text = row['text']\n",
    "    # legal_clauses = row['Legal_Clauses_Found'] # for precedents replace to 'Precedents_found'\n",
    "    legal_clauses = row['Precedents_Found'] \n",
    "    masked_text = text\n",
    "    \n",
    "    for clause in legal_clauses:\n",
    "        if clause in text:\n",
    "            # Replace each clause with the <LEGAL_CLAUSE> placeholder\n",
    "            masked_text = masked_text.replace(clause, '<LEGAL_CLAUSE>')\n",
    "    \n",
    "    return masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column 'masked_text'\n",
    "filtered_Hugging_Face_df['masked_text'] = filtered_Hugging_Face_df.apply(create_masked_text, axis=1)\n",
    "filtered_Hugging_Face_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a special token for legal clause generation\n",
    "special_tokens_dict = {'additional_special_tokens': ['<LEGAL_CLAUSE>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"masked_text\"]\n",
    "    targets = examples[\"text\"]\n",
    "    \n",
    "    # Tokenize the inputs (masked text with special token)\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize the targets (original text)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  # Set the original text as labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(filtered_Hugging_Face_df)\n",
    "\n",
    "# Apply the preprocessing function\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and evaluation\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Text2Text_Results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,  # Reduced batch size\n",
    "    per_device_eval_batch_size=4,   # Reduce evaluation batch size as well\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Enable mixed precision to reduce memory usage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    trainer.train() # (37 Hours)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from last checkpoint\n",
    "trainer.train(resume_from_checkpoint=\"./Text2Text_Results/checkpoint-184400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save The Model\n",
    "trainer.save_model(\"Text2Text_finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Legal_Clauses fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"shay681/Text2Text_Legal_Clauses_finetuned_model\")\n",
    "model.to(device)  # Move model to GPU\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"shay681/Text2Text_Legal_Clauses_finetuned_model\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "# Load the Precedents fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"shay681/Text2Text_Precedents_finetuned_model\")\n",
    "model.to(device)  # Move model to GPU\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"shay681/Text2Text_Precedents_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split df\n",
    "train_df, eval_df = train_test_split(filtered_Hugging_Face_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = eval_df[eval_df['text'].str.len() < 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_decoding_for_multiple_clauses(model, tokenizer, input_text, allowed_clauses, max_length=50):\n",
    "    # Split the text by the <LEGAL_CLAUSE> placeholders\n",
    "    parts = input_text.split('<LEGAL_CLAUSE>')\n",
    "    \n",
    "    # Initialize the final generated text\n",
    "    generated_text = parts[0]\n",
    "    \n",
    "    for i in range(1, len(parts)):\n",
    "        # Prepare the input text with the current generated text and the next placeholder\n",
    "        current_input = generated_text + \"<extra_id_0>\" + parts[i]\n",
    "\n",
    "        # Tokenize the input text\n",
    "        input_ids = tokenizer.encode(current_input, return_tensors=\"pt\").to(device) \n",
    "        \n",
    "        # Use no_grad for inference\n",
    "        with torch.no_grad():\n",
    "            # Generate output from the model with increased diversity\n",
    "            generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=1.0,  # Increase temperature to introduce more randomness\n",
    "                top_k=30,  # Consider the top 30 tokens to introduce more variability\n",
    "                top_p=0.95,  # Use nucleus sampling for more diverse outputs\n",
    "                do_sample=True  # Enable sampling to avoid repetitive outputs\n",
    "            )\n",
    "        \n",
    "        # Decode the generated output\n",
    "        generated_clause_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Tokenize the generated text\n",
    "        generated_tokens = tokenizer.encode(generated_clause_text, return_tensors=\"pt\").squeeze(0)\n",
    "\n",
    "        best_match = None\n",
    "        best_score = float(\"-inf\")  # Use negative infinity for maximization\n",
    "\n",
    "        for clause in allowed_clauses:\n",
    "            # Tokenize each clause\n",
    "            clause_tokens = tokenizer.encode(clause, return_tensors=\"pt\").squeeze(0)\n",
    "            \n",
    "            # Make sure the generated tokens and clause tokens are the same size\n",
    "            min_length = min(generated_tokens.size(0), clause_tokens.size(0))\n",
    "            generated_tokens_truncated = generated_tokens[:min_length]\n",
    "            clause_tokens_truncated = clause_tokens[:min_length]\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            score = torch.nn.functional.cosine_similarity(\n",
    "                generated_tokens_truncated.float(),\n",
    "                clause_tokens_truncated.float(),\n",
    "                dim=0\n",
    "            ).item()\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = clause\n",
    "\n",
    "        # Replace <LEGAL_CLAUSE> with the best match clause\n",
    "        generated_text += best_match + parts[i]\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for constrained decoding across multiple rows in the dataframe\n",
    "def generate_clauses_for_dataframe(df, model, tokenizer, allowed_clauses_col, input_col):\n",
    "    # Initialize an empty list to store generated clauses\n",
    "    generated_clauses = []\n",
    "    \n",
    "    # Loop through each row in the dataframe with progress tracking\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Extract the input text and allowed clauses\n",
    "        input_text = row[input_col]\n",
    "        allowed_clauses = row[allowed_clauses_col]\n",
    "\n",
    "        # Perform constrained decoding\n",
    "        generated_text = constrained_decoding_for_multiple_clauses(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            input_text=input_text,\n",
    "            allowed_clauses=allowed_clauses\n",
    "        )\n",
    "\n",
    "        # Append the generated text to the list\n",
    "        generated_clauses.append(generated_text)\n",
    "\n",
    "    # Return the list of generated clauses\n",
    "    return generated_clauses\n",
    "\n",
    "# Use the function the test_df\n",
    "test_df = sample_df\n",
    "\n",
    "# Generate clauses for each row and add a new column to the DataFrame\n",
    "test_df['generated_clauses'] = generate_clauses_for_dataframe(\n",
    "    test_df, model, tokenizer, allowed_clauses_col='Legal_Clauses_Found', input_col='masked_text'\n",
    ")\n",
    "# For Precendents allowed_clauses_col='Precedents_Found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_text</th>\n",
       "      <th>generated_clauses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>651063</th>\n",
       "      <td>בבית המשפט העליון    ...</td>\n",
       "      <td>בבית המשפט העליון    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639497</th>\n",
       "      <td>בבית המשפט העליון    ...</td>\n",
       "      <td>בבית המשפט העליון    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650671</th>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100714</th>\n",
       "      <td>בבית המשפט העליון  בשבתו כבית משפט ...</td>\n",
       "      <td>בבית המשפט העליון  בשבתו כבית משפט ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666840</th>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661998</th>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516376</th>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185274</th>\n",
       "      <td>בבית המשפט העליון  בירושלי...</td>\n",
       "      <td>בבית המשפט העליון  בירושלי...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616858</th>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645961</th>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "      <td>בבית המשפט העליו...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104226 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              masked_text  \\\n",
       "651063                           בבית המשפט העליון    ...   \n",
       "639497                           בבית המשפט העליון    ...   \n",
       "650671                                בבית המשפט העליו...   \n",
       "100714             בבית המשפט העליון  בשבתו כבית משפט ...   \n",
       "666840                                בבית המשפט העליו...   \n",
       "...                                                   ...   \n",
       "661998                                בבית המשפט העליו...   \n",
       "516376                                בבית המשפט העליו...   \n",
       "185274                      בבית המשפט העליון  בירושלי...   \n",
       "616858                                בבית המשפט העליו...   \n",
       "645961                                בבית המשפט העליו...   \n",
       "\n",
       "                                        generated_clauses  \n",
       "651063                           בבית המשפט העליון    ...  \n",
       "639497                           בבית המשפט העליון    ...  \n",
       "650671                                בבית המשפט העליו...  \n",
       "100714             בבית המשפט העליון  בשבתו כבית משפט ...  \n",
       "666840                                בבית המשפט העליו...  \n",
       "...                                                   ...  \n",
       "661998                                בבית המשפט העליו...  \n",
       "516376                                בבית המשפט העליו...  \n",
       "185274                      בבית המשפט העליון  בירושלי...  \n",
       "616858                                בבית המשפט העליו...  \n",
       "645961                                בבית המשפט העליו...  \n",
       "\n",
       "[104226 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output the DataFrame with generated clauses for review\n",
    "test_df[['masked_text', 'generated_clauses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Legal_Clauses df to a parquet file\n",
    "test_df.to_parquet('Inference_Legal_Clauses_allowed_clauses.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "# OR\n",
    "\n",
    "# Save Precedents df to a parquet file\n",
    "test_df.to_parquet('Inference_Precedents_allowed_clauses.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 89.0M/89.0M [00:16<00:00, 5.46MB/s]\n",
      "Generating train split: 26677 examples [00:00, 39269.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset package\n",
    "dataset = load_dataset(\"shay681/Inference_Legal_Clauses\")\n",
    "\n",
    "# Convert the datasets to Dataframe\n",
    "Legal_Clauses_eval_dataset = pd.DataFrame.from_dict(dataset['train'])\n",
    "\n",
    "# Load locally\n",
    "# Legal_Clauses_eval_dataset = pd.read_parquet('Inference_Legal_Clauses_allowed_clauses.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 253M/253M [00:41<00:00, 6.05MB/s] \n",
      "Generating train split: 104226 examples [00:01, 59713.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset package\n",
    "dataset = load_dataset(\"shay681/Inference_Precedents\")\n",
    "\n",
    "# Convert the datasets to Dataframe\n",
    "Precedents_eval_dataset = pd.DataFrame.from_dict(dataset['train'])\n",
    "\n",
    "# Load locally\n",
    "# Precedents_eval_dataset = pd.read_parquet('Inference_Precedents_allowed_clauses.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Remove punctuation and unnecessary characters, normalize spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation (non-alphanumeric)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize multiple spaces to a single space\n",
    "    text = text.strip()  # Trim leading/trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(random_sample, doc_type):\n",
    "    total_clauses = 0\n",
    "    correct_clauses = 0\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for _, row in random_sample.iterrows():\n",
    "        found_clauses = row[doc_type + '_Found']  # This is a list of clauses\n",
    "        generated_clause = row['generated_clauses']  # This is a string of generated text\n",
    "\n",
    "        # Count the total clauses\n",
    "        total_clauses += len(found_clauses)\n",
    "\n",
    "        # Normalize generated_clause\n",
    "        normalized_generated_clause = normalize_text(generated_clause)\n",
    "\n",
    "        # Check if found_clauses are in the normalized_generated_clause\n",
    "        for clause in found_clauses:\n",
    "            if normalize_text(clause) in normalized_generated_clause:\n",
    "                correct_clauses += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_clauses / total_clauses if total_clauses > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal_Clauses Accuracy: 87.88%\n"
     ]
    }
   ],
   "source": [
    "# Calculate Legal_Clauses accuracy\n",
    "accuracy = calculate_accuracy(Legal_Clauses_eval_dataset, 'Legal_Clauses')\n",
    "print(f'Legal_Clauses Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precedents Accuracy: 7.51%\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precedents accuracy\n",
    "accuracy = calculate_accuracy(Precedents_eval_dataset, 'Precedents')\n",
    "print(f'Precedents Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matching_clauses(generated_clause, found_clauses):\n",
    "    # Normalize the generated clause\n",
    "    normalized_generated_clause = normalize_text(generated_clause)\n",
    "    \n",
    "    # Initialize a list to hold matched clauses\n",
    "    matching_clauses = []\n",
    "\n",
    "    # Iterate through each found clause\n",
    "    for clause in found_clauses:\n",
    "        normalized_clause = normalize_text(clause)\n",
    "        # Check if the normalized clause is in the normalized generated clause\n",
    "        if normalized_clause in normalized_generated_clause:\n",
    "            matching_clauses.append(normalized_clause)\n",
    "\n",
    "    return matching_clauses\n",
    "\n",
    "def calculate_bleu_score(found_clauses, generated_clause):\n",
    "    # Extract matching clauses from generated_clause\n",
    "    matching_clauses = extract_matching_clauses(generated_clause, found_clauses)\n",
    "\n",
    "    # Initialize a list to store BLEU scores for each matched clause\n",
    "    bleu_scores = []\n",
    "\n",
    "    # Iterate through each matching clause\n",
    "    for matching_clause in matching_clauses:\n",
    "        reference_tokens = normalize_text(matching_clause).split()  # Tokenize the matched clause\n",
    "        for clause in found_clauses:\n",
    "            generated_tokens = (normalize_text(clause).split())  # Tokenize the generated clause\n",
    "        \n",
    "        # Calculate BLEU score for the matched clause against the found clause\n",
    "        bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # Return the average BLEU score across all matched clauses\n",
    "    if bleu_scores:\n",
    "        return sum(bleu_scores) / len(bleu_scores)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.888085759821180\n"
     ]
    }
   ],
   "source": [
    "# Apply the bleu_score function on Legal Clauses\n",
    "average_bleu_score = 0\n",
    "total_rows = len(Legal_Clauses_eval_dataset)\n",
    "\n",
    "for _, row in Legal_Clauses_eval_dataset.iterrows():\n",
    "    found_clauses = row['Legal_Clauses_Found']  # List of legal clauses\n",
    "    generated_clause = row['generated_clauses']  # String of generated text\n",
    "\n",
    "    # Calculate BLEU score for each row\n",
    "    average_bleu_score += calculate_bleu_score(found_clauses, generated_clause)\n",
    "\n",
    "# Final average BLEU score\n",
    "average_bleu_score /= total_rows\n",
    "print(f'Average BLEU Score: {average_bleu_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Apply the bleu_score function on Precedents\n",
    "average_bleu_score = 0\n",
    "total_rows = len(Precedents_eval_dataset)\n",
    "\n",
    "for _, row in Precedents_eval_dataset.iterrows():\n",
    "    found_clauses = row['Precedents_Found']  # List of legal clauses\n",
    "    generated_clause = row['generated_clauses']  # String of generated text\n",
    "\n",
    "    # Calculate BLEU score for each row\n",
    "    average_bleu_score += calculate_bleu_score(found_clauses, generated_clause)\n",
    "\n",
    "# Final average BLEU score\n",
    "average_bleu_score /= total_rows\n",
    "print(f'Average BLEU Score: {average_bleu_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_f1(found_clauses, generated_clause):\n",
    "    # Extract matching clauses from generated_clause\n",
    "    matching_clauses = extract_matching_clauses(generated_clause, found_clauses)\n",
    "\n",
    "    # True Positives: correctly predicted clauses\n",
    "    true_positives = len(matching_clauses)\n",
    "\n",
    "    # False Negatives: found_clauses that were not in generated_clause\n",
    "    false_negatives = len([clause for clause in found_clauses if clause not in matching_clauses])\n",
    "\n",
    "    # Calculate Recall\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    # Calculate Precision\n",
    "    precision = true_positives / len(matching_clauses) if len(matching_clauses) > 0 else 0\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_recall_f1(dataset, docType):\n",
    "    # Iterate through each row in the DataFrame and calculate the average Recall and F1 Score\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_rows = len(dataset)\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        found_clauses = row[docType]  # List of legal clauses\n",
    "        generated_clause = row['generated_clauses']  # String of generated text\n",
    "\n",
    "        # Calculate Recall and F1 score for each row\n",
    "        recall, f1 = calculate_recall_f1(found_clauses, generated_clause)\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "\n",
    "    return total_recall, total_f1, total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Recall and F1-Score\n",
    "Legal_Clauses_total_recall, Legal_Clauses_total_f1, Legal_Clauses_total_rows = total_recall_f1(Legal_Clauses_eval_dataset, 'Legal_Clauses_Found')\n",
    "Precedents_total_recall, Precedents_total_f1, Precedents_total_rows = total_recall_f1(Precedents_eval_dataset, 'Precedents_Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall of Legal_Clauses: 0.478951767009621\n",
      "Average F1 Score of Legal_Clauses: 0.644487282073813\n"
     ]
    }
   ],
   "source": [
    "# Final average Recall and F1 Score for Legal_Clauses\n",
    "Legal_Clauses_average_recall = Legal_Clauses_total_recall / Legal_Clauses_total_rows\n",
    "Legal_Clauses_average_f1_score = Legal_Clauses_total_f1 / Legal_Clauses_total_rows\n",
    "\n",
    "print(f'Average Recall of Legal_Clauses: {Legal_Clauses_average_recall:.3f}')\n",
    "print(f'Average F1 Score of Legal_Clauses: {Legal_Clauses_average_f1_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall of Precedents: 0.017929827979265\n",
      "Average F1 Score of Precedents: 0.024801429867562\n"
     ]
    }
   ],
   "source": [
    "# Final average Recall and F1 Score for Precedents\n",
    "Precedents_average_recall = Precedents_total_recall / Precedents_total_rows\n",
    "Precedents_average_f1_score = Precedents_total_f1 / Precedents_total_rows\n",
    "\n",
    "print(f'Average Recall of Precedents: {Precedents_average_recall:.3f}')\n",
    "print(f'Average F1 Score of Precedents: {Precedents_average_f1_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROUGE score for a single row\n",
    "def calculate_rouge_scores(row, doc_type):\n",
    "    found_clauses = row[doc_type]  # This is a list of clauses\n",
    "    generated_clause = row['generated_clauses']  # This is a string of generated text\n",
    "\n",
    "    # Initialize the rouge scorer for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Initialize accumulators for averaging scores across multiple found clauses\n",
    "    rouge1_f, rouge1_p, rouge1_r = 0, 0, 0\n",
    "    rouge2_f, rouge2_p, rouge2_r = 0, 0, 0\n",
    "    rougeL_f, rougeL_p, rougeL_r = 0, 0, 0\n",
    "    \n",
    "    # Track the number of valid clauses\n",
    "    num_clauses = len(found_clauses)\n",
    "\n",
    "    for clause in found_clauses:\n",
    "        # Calculate the ROUGE scores for each clause in found_clauses\n",
    "        scores = scorer.score(clause, generated_clause)\n",
    "        \n",
    "        # Accumulate the scores for averaging later\n",
    "        rouge1_f += scores['rouge1'].fmeasure\n",
    "        rouge1_p += scores['rouge1'].precision\n",
    "        rouge1_r += scores['rouge1'].recall\n",
    "\n",
    "        rouge2_f += scores['rouge2'].fmeasure\n",
    "        rouge2_p += scores['rouge2'].precision\n",
    "        rouge2_r += scores['rouge2'].recall\n",
    "\n",
    "        rougeL_f += scores['rougeL'].fmeasure\n",
    "        rougeL_p += scores['rougeL'].precision\n",
    "        rougeL_r += scores['rougeL'].recall\n",
    "\n",
    "    # Avoid division by zero in case there are no clauses\n",
    "    if num_clauses > 0:\n",
    "        return {\n",
    "            'rouge1_fmeasure': rouge1_f / num_clauses,\n",
    "            'rouge1_precision': rouge1_p / num_clauses,\n",
    "            'rouge1_recall': rouge1_r / num_clauses,\n",
    "            'rouge2_fmeasure': rouge2_f / num_clauses,\n",
    "            'rouge2_precision': rouge2_p / num_clauses,\n",
    "            'rouge2_recall': rouge2_r / num_clauses,\n",
    "            'rougeL_fmeasure': rougeL_f / num_clauses,\n",
    "            'rougeL_precision': rougeL_p / num_clauses,\n",
    "            'rougeL_recall': rougeL_r / num_clauses\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'rouge1_fmeasure': 0, 'rouge1_precision': 0, 'rouge1_recall': 0,\n",
    "            'rouge2_fmeasure': 0, 'rouge2_precision': 0, 'rouge2_recall': 0,\n",
    "            'rougeL_fmeasure': 0, 'rougeL_precision': 0, 'rougeL_recall': 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal_Clauses - Average ROUGE Scores:\n",
      "ROUGE-1 F1: 0.0687\n",
      "ROUGE-1 Precision: 0.0360\n",
      "ROUGE-1 Recall: 0.9480\n",
      "ROUGE-2 F1: 0.0000\n",
      "ROUGE-2 Precision: 0.0000\n",
      "ROUGE-2 Recall: 0.0000\n",
      "ROUGE-L F1: 0.0687\n",
      "ROUGE-L Precision: 0.0360\n",
      "ROUGE-L Recall: 0.9480\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to calculate ROUGE scores for each row, passing 'doc_type' as an argument\n",
    "Legal_Clauses_rouge_scores = Legal_Clauses_eval_dataset.apply(lambda row: calculate_rouge_scores(row, 'Legal_Clauses_Found'), axis=1)\n",
    "\n",
    "# Convert the resulting list of dictionaries into a DataFrame\n",
    "Legal_Clauses_rouge_scores_df = pd.DataFrame(Legal_Clauses_rouge_scores.tolist())\n",
    "\n",
    "# Calculate the average ROUGE scores across all rows\n",
    "Legal_Clauses_average_rouge_scores = Legal_Clauses_rouge_scores_df.mean()\n",
    "\n",
    "# Print the Legal_Clauses average scores\n",
    "print(\"Legal_Clauses - Average ROUGE Scores:\")\n",
    "print(f\"ROUGE-1 F1: {Legal_Clauses_average_rouge_scores['rouge1_fmeasure']:.4f}\")\n",
    "print(f\"ROUGE-1 Precision: {Legal_Clauses_average_rouge_scores['rouge1_precision']:.4f}\")\n",
    "print(f\"ROUGE-1 Recall: {Legal_Clauses_average_rouge_scores['rouge1_recall']:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {Legal_Clauses_average_rouge_scores['rouge2_fmeasure']:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {Legal_Clauses_average_rouge_scores['rouge2_precision']:.4f}\")\n",
    "print(f\"ROUGE-2 Recall: {Legal_Clauses_average_rouge_scores['rouge2_recall']:.4f}\")\n",
    "print(f\"ROUGE-L F1: {Legal_Clauses_average_rouge_scores['rougeL_fmeasure']:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {Legal_Clauses_average_rouge_scores['rougeL_precision']:.4f}\")\n",
    "print(f\"ROUGE-L Recall: {Legal_Clauses_average_rouge_scores['rougeL_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precedents - Average ROUGE Scores:\n",
      "ROUGE-1 F1: 0.1599\n",
      "ROUGE-1 Precision: 0.0887\n",
      "ROUGE-1 Recall: 0.9666\n",
      "ROUGE-2 F1: 0.0878\n",
      "ROUGE-2 Precision: 0.0466\n",
      "ROUGE-2 Recall: 0.9448\n",
      "ROUGE-L F1: 0.1599\n",
      "ROUGE-L Precision: 0.0887\n",
      "ROUGE-L Recall: 0.9665\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to calculate ROUGE scores for each row, passing 'doc_type' as an argument\n",
    "Precedents_rouge_scores = Precedents_eval_dataset.apply(lambda row: calculate_rouge_scores(row, 'Precedents_Found'), axis=1)\n",
    "\n",
    "# Convert the resulting list of dictionaries into a DataFrame\n",
    "Precedents_rouge_scores_df = pd.DataFrame(Precedents_rouge_scores.tolist())\n",
    "\n",
    "# Calculate the average ROUGE scores across all rows\n",
    "Precedents_average_rouge_scores = Precedents_rouge_scores_df.mean()\n",
    "\n",
    "# Print the Precedents average scores\n",
    "print(\"Precedents - Average ROUGE Scores:\")\n",
    "print(f\"ROUGE-1 F1: {Precedents_average_rouge_scores['rouge1_fmeasure']:.4f}\")\n",
    "print(f\"ROUGE-1 Precision: {Precedents_average_rouge_scores['rouge1_precision']:.4f}\")\n",
    "print(f\"ROUGE-1 Recall: {Precedents_average_rouge_scores['rouge1_recall']:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {Precedents_average_rouge_scores['rouge2_fmeasure']:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {Precedents_average_rouge_scores['rouge2_precision']:.4f}\")\n",
    "print(f\"ROUGE-2 Recall: {Precedents_average_rouge_scores['rouge2_recall']:.4f}\")\n",
    "print(f\"ROUGE-L F1: {Precedents_average_rouge_scores['rougeL_fmeasure']:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {Precedents_average_rouge_scores['rougeL_precision']:.4f}\")\n",
    "print(f\"ROUGE-L Recall: {Precedents_average_rouge_scores['rougeL_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained SBERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to calculate semantic similarity between Legal_Clauses_Found and generated_clauses\n",
    "def calculate_semantic_similarity(row, doc_type):\n",
    "    found_clauses = row[doc_type]  # List of clauses\n",
    "    generated_clause = row['generated_clauses']  # String of generated text\n",
    "    \n",
    "    # Encode the found clauses and generated clauses into embeddings\n",
    "    found_clauses_embedding = model.encode(found_clauses, convert_to_tensor=True).cpu()\n",
    "    generated_clause_embedding = model.encode([generated_clause], convert_to_tensor=True).cpu()\n",
    "    \n",
    "    # Calculate cosine similarity between the found clauses and generated clause embeddings\n",
    "    similarities = cosine_similarity(found_clauses_embedding, generated_clause_embedding)\n",
    "    \n",
    "    # Average the similarities\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    \n",
    "    return avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Semantic Similarity: 0.8072\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the DataFrame (13 Min)\n",
    "Legal_Clauses_eval_dataset['semantic_similarity'] = Legal_Clauses_eval_dataset.apply(lambda row: calculate_semantic_similarity(row, 'Legal_Clauses_Found'), axis=1)\n",
    "\n",
    "# Calculate the average semantic similarity\n",
    "average_semantic_similarity = Legal_Clauses_eval_dataset['semantic_similarity'].mean()\n",
    "\n",
    "print(f'Average Semantic Similarity: {average_semantic_similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Semantic Similarity: 0.4095\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the DataFrame (13 Min)\n",
    "Precedents_eval_dataset['semantic_similarity'] = Precedents_eval_dataset.apply(lambda row: calculate_semantic_similarity(row, 'Precedents_Found'), axis=1)\n",
    "\n",
    "# Calculate the average semantic similarity\n",
    "average_semantic_similarity = Precedents_eval_dataset['semantic_similarity'].mean()\n",
    "\n",
    "print(f'Average Semantic Similarity: {average_semantic_similarity:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
